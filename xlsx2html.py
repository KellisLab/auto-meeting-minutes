"""
xlsx2html.py - Convert Excel transcript files to HTML with links and summaries
Usage: python xlsx2html.py input.xlsx video_id [output.html] [--format={simple|numbered}]

This script takes an Excel file generated by txt2xlsx.py and:
1. Creates direct links to video timestamps for all speakers
2. Generates AI-powered summaries for transcript sections
3. Places the summaries near their corresponding speaker links
4. Generates separate HTML and Markdown files with:
  - Speaker summaries with direct timestamp links
  - Meeting summaries organized by batch with links to both batch start times and
    individual topic timestamps within each batch
  - Topic-level navigation within meeting summaries

Examples:
python xlsx2html.py meeting.xlsx 757a2c7c-eb52-47d1-9b4a-b2a1014b530b
python xlsx2html.py meeting.xlsx 757a2c7c-eb52-47d1-9b4a-b2a1014b530b meeting_links.html
"""

import sys
import os
import pandas as pd
import argparse
import re
import time
import json
import openai
from dotenv import load_dotenv
import numpy as np
import importlib.util
import requests
import base64
from io import BytesIO
import io 
from utils import screenshot_panopto_as_image


# Import utility functions from utils.py
from utils import (
    seconds_to_time_str,
    time_str_to_seconds,
    format_corrected_timestamp,
    verify_timestamp_format,
    get_column_letter,
    extract_transcript_data,
    extract_unique_speakers,
    create_time_batches,
    extract_text_for_batch,
    find_best_timestamp_match,
    update_speaker_timestamps_for_topics,
    extract_topics_from_summary,
    get_api_key,
)

from speaker_summary_utils import (
    enhance_speaker_tracking,
    summarize_speaker_topic,
    generate_enhanced_speaker_summary_markdown,
    generate_enhanced_speaker_summary_html,
    generate_speaker_summaries_data,
)

from PIL import Image
import io


# -------------------------------------------------------------
# Constants and Configuration
# -------------------------------------------------------------
# load API KEY from .env
load_dotenv()
# Access the API key
OPENAI_API_KEY = os.getenv("API_KEY")
MODEL = os.getenv("GPT_MODEL", "gpt-4o")
# Default batch size for meeting summaries (in minutes)
DEFAULT_BATCH_SIZE_MINUTES = 40
ENHANCED_SUMMARIES_AVAILABLE = True
RESIZE_SCALE = 0.5 # constant to resize the images to 50% of their original size


# -------------------------------------------------------------
# HTML and Markdown Generation Functions
# -------------------------------------------------------------
def generate_meeting_summaries_html(
    batches, batch_summaries, video_id, html_file, transcript_data=None, images_to_be_embedded_per_topic=None
):
    """
    Generate HTML file with meeting batch summaries that include clickable timestamp links
    for both batches and individual topics within each batch.
    Topics are sorted chronologically by timestamp across all batches.

    Args:
        batches (list): List of batch entries
        batch_summaries (list): List of batch summaries
        video_id (str): Panopto video ID
        html_file (str): Output HTML file path
        transcript_data (list, optional): Full transcript data for better timestamp matching
        images_to_be_embedded_per_topic (dict, optional): mapping each topic to a list of images to be embedded for that topic

    Returns:
        str: Path to the generated HTML file
    """
    # Initialize separate lists for heading and list items
    heading_html = ""
    list_items_html = []

    try:
        title = re.sub(r'(?<=\d)\.(\d{2})(am|pm)', r':\1\2', html_file)
        folder_name = os.path.basename(os.path.dirname(title))
        formatted_name = folder_name.replace("_", " ")
        video_link = (
            f"https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id={video_id}"
        )
        # Modified title with (link) in blue - now stored separately from list items
        heading_html = f'<h1><a href="{video_link}">{formatted_name} <span style="color: #1155cc;">(link)</span></a></h1>'
    except:
        heading_html = "<h1>Meeting Summary</h1>"

    # Extract all topics from all batches
    all_topics = []

    for i, (batch, summary) in enumerate(zip(batches, batch_summaries), 1):
        # Extract topics from the summary with their timestamps
        topics = extract_topics_from_summary(summary, video_id, transcript_data)

        # If we have transcript data, update the topic timestamps to better match content
        if transcript_data:
            topics = update_speaker_timestamps_for_topics(topics, transcript_data)

        # Add batch index for reference
        for topic in topics:
            topic["batch_index"] = i
            topic["batch"] = batch

        all_topics.extend(topics)

    # Sort all topics by timestamp_seconds
    all_topics.sort(
        key=lambda x: (
            x["timestamp_seconds"]
            if x["timestamp_seconds"] is not None
            else float("inf")
        )
    )

    # Process each topic
    for idx, topic_info in enumerate(all_topics, 1):
        topic = topic_info["topic"]
        speaker = topic_info["speaker"]
        content = topic_info["content"]
        batch = topic_info["batch"]

        # Check if the topic has a direct timestamp link
        if topic_info["video_link"] and topic_info["timestamp_seconds"] is not None:
            # Use the direct link from the timestamp in the summary
            topic_link = topic_info["video_link"]
            seconds = topic_info["timestamp_seconds"]

            # Verify the timestamp matches the seconds value
            # If not, get a corrected timestamp
            corrected_timestamp = verify_timestamp_format(
                topic_info["timestamp"], seconds
            )

            # Add topic as a numbered list item with direct link and corrected timestamp in blue
            list_items_html.append(
                f'<li><h3 class="topic-heading"><a href="{topic_link}" class="topic-link">{topic} - {speaker} <span style="color: #1155cc;">({corrected_timestamp})</span></a></h3>'
            )
        else:
            # Fallback: Find the entry for this speaker in the batch
            names = re.split(r'\s*&\s*|,\s*| and ', speaker)
            speaker_entry = None
            for entry in batch:
                if entry["name"] in names:
                    speaker_entry = entry
                    break

            # If we found the entry, create a link to it
            if speaker_entry:
                speaker_seconds = speaker_entry["seconds"]
                speaker_time = verify_timestamp_format(
                    speaker_entry.get("time_str", ""), speaker_seconds
                )
                topic_link = f"https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id={video_id}&start={speaker_seconds}"

                # Add topic as a numbered list item with link from entry and blue timestamp
                list_items_html.append(
                    f'<li><h3 class="topic-heading"><a href="{topic_link}" class="topic-link">{topic} - {speaker} <span style="color: #1155cc;">({speaker_time})</span></a></h3>'
                )
            else:
                # If no entry found, just display the topic without a link
                list_items_html.append(
                    f'<li><h3 class="topic-heading">{topic} - {speaker}</h3>'
                )

        # Add the content for this topic
        list_items_html.append(f'<div class="topic-content">{content}</div></li>')

        # only if there are images for this topic
        key = f"{topic} - {speaker}"
        accumulated_descriptions = []

        if images_to_be_embedded_per_topic and key in images_to_be_embedded_per_topic:
            list_items_html.append('<h4 class="topic-heading">Images</h4>')
            for img in images_to_be_embedded_per_topic[key]:
                # correctly Base64‐encode
                b64 = base64.b64encode(img['bytes']).decode('ascii')
                # use double-quotes for the f‐string so we can single‐quote HTML attributes
                list_items_html.append(
                    f"<img src='data:image/jpeg;base64,{b64}' "
                    f"alt='{img['timestamp']}'/>"
                )
                # Add description as bullet point if it exists
                if 'description' in img:
                    accumulated_descriptions.append(img['description'])

        if accumulated_descriptions:
            list_items_html.append('<h4 class="topic-heading">Descriptions</h4>')
            for description in accumulated_descriptions:
                list_items_html.append(f"<ul><li>{description}</li></ul>")


    # Combine all lines into HTML
    html_content = "<!DOCTYPE html>\n<html>\n<head>\n<title>Meeting Summaries</title>\n"
    html_content += "<style>\n"
    html_content += (
        "body { font-family: Arial, sans-serif; margin: 20px; font-size: 11px; }\n"
    )
    html_content += (
        "ol { list-style-position: outside; padding-left: 12px; margin-top: 10px; }\n"
    )
    html_content += "ol li { margin-bottom: 1px; }\n"
    html_content += ".topic-content { margin-bottom: 0px; font-family: Arial, sans-serif; font-size: 11px; margin-top: 0px; }\n"
    # Title styling - Cambria, 11px, #c0504d, underlined
    html_content += "h1 { font-family: Cambria, serif; font-size: 11px; color: #c0504d; text-decoration: underline; }\n"
    html_content += "h1 a { color: #c0504d; text-decoration: underline; }\n"
    # Topic styling - Arial, 11px, #7030a0, underlined
    html_content += "h3.topic-heading { font-family: Arial, sans-serif; font-size: 11px; color: #7030a0; text-decoration: underline; margin-top: 0px; margin-bottom: 1px; }\n"
    html_content += "a { color: inherit; }\n"
    html_content += ".topic-link { text-decoration: underline; color: #7030a0; }\n"
    html_content += ".topic-link span { text-decoration: underline; }\n"
    html_content += "b { font-weight: bold; }\n"
    html_content += "</style>\n</head>\n<body>\n"

    # Add the heading BEFORE the ordered list
    html_content += heading_html + "\n"

    # Wrap list items in an ordered list
    html_content += "<ol>\n"
    html_content += "\n".join(list_items_html)
    html_content += "\n</ol>\n</body>\n</html>"

    # Write the file
    with open(html_file, "w", encoding="utf-8") as f:
        f.write(html_content)

    print(f"Generated meeting summaries HTML with verified timestamps: {html_file}")
    return html_file


def generate_meeting_summaries_markdown(
    batches, batch_summaries, video_id, md_file, transcript_data=None, images_to_be_embedded_per_topic=None
):
    """
    Generate Markdown file with meeting batch summaries that include clickable timestamp links
    for both batches and individual topics within each batch.
    Topics are sorted chronologically by timestamp, and timestamps are verified to match URL seconds.

    Args:
        batches (list): List of batch entries
        batch_summaries (list): List of batch summaries
        video_id (str): Panopto video ID
        md_file (str): Output Markdown file path
        transcript_data (list, optional): Full transcript data for better timestamp matching
        images_to_be_embedded_per_topic (dict, optional): mapping each topic to a list of images to be embedded for that topic

    Returns:
        str: Path to the generated Markdown file
    """
    md_lines = []
    try:
        title = re.sub(r'(?<=\d)\.(\d{2})(am|pm)', r':\1\2', md_file)
        folder_name = os.path.basename(os.path.dirname(title))
        formatted_name = folder_name.replace("_", " ")
        video_link = (
            f"https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id={video_id}"
        )
        md_lines.append(f"# [{formatted_name}]({video_link})\n")
    except:
        md_lines.append("# Meeting Summary\n")
    # Extract all topics from all batches
    all_topics = []

    for i, (batch, summary) in enumerate(zip(batches, batch_summaries), 1):
        # Extract topics from the summary with their timestamps
        topics = extract_topics_from_summary(summary, video_id, transcript_data)

        # If we have transcript data, update the topic timestamps to better match content 
        # only modifies timestamp_seconds and timestamp
        if transcript_data:
            topics = update_speaker_timestamps_for_topics(topics, transcript_data)

        # Add batch index for reference
        for topic in topics:
            topic["batch_index"] = i
            topic["batch"] = batch


        # TODO potentially find 5 candidates based on each batch and topics and then select up to 3 images 
        # create a dictionary of topics to up to 3 images 

        all_topics.extend(topics)

    # Sort all topics by timestamp_seconds
    all_topics.sort(
        key=lambda x: (
            x["timestamp_seconds"]
            if x["timestamp_seconds"] is not None
            else float("inf")
        )
    )

    # Process each topic
    for topic_info in all_topics:
        topic = topic_info["topic"]
        speaker = topic_info["speaker"]
        content = topic_info["content"]
        batch = topic_info["batch"]

        # Check if the topic has a direct timestamp link
        if topic_info["video_link"] and topic_info["timestamp_seconds"] is not None:
            # Use the direct link from the timestamp in the summary
            topic_link = topic_info["video_link"]
            seconds = topic_info["timestamp_seconds"]

            # Verify the timestamp matches the seconds value
            # If not, get a corrected timestamp
            corrected_timestamp = verify_timestamp_format(
                topic_info["timestamp"], seconds
            )

            # Add topic as a subheading with direct link and corrected timestamp
            md_lines.append(
                f"**{topic} - {speaker}** [({corrected_timestamp})]({topic_link})"
            )
        else:
            # Fallback: Find the entry for this speaker in the batch
            speaker_entry = None
            for entry in batch:
                if entry["name"] == speaker:
                    speaker_entry = entry
                    break

            # If we found the entry, create a link to it
            if speaker_entry:
                speaker_seconds = speaker_entry["seconds"]
                speaker_time = verify_timestamp_format(
                    speaker_entry.get("time_str", ""), speaker_seconds
                )
                topic_link = f"https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id={video_id}&start={speaker_seconds}"

                # Add topic as a subheading with link from entry
                md_lines.append(
                    f"### {topic} - {speaker} [({speaker_time})]({topic_link})"
                )
            else:
                # If no entry found, just display the topic without a link
                md_lines.append(f"### {topic} - {speaker}\n")

        # Add the content for this topic
        md_lines.append(f"{content}\n")

        accumulated_descriptions = []

        key = f"{topic} - {speaker}"
        if images_to_be_embedded_per_topic and key in images_to_be_embedded_per_topic:
            md_lines.append("*Images to be embedded*\n")
            for image in images_to_be_embedded_per_topic[key]:
                b64 = base64.b64encode(image['bytes']).decode('ascii')
                md_lines.append(f"![{image['timestamp']}](data:image/jpeg;base64,{b64})")
                # Add description as bullet point if it exists
                if 'description' in image:
                    accumulated_descriptions.append(image['description'])

        if accumulated_descriptions:
            md_lines.append("*Descriptions*\n")
            for description in accumulated_descriptions:
                md_lines.append(f"* {description}")
        md_lines.append("\n")

    # Write the file
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))

    print(f"Generated meeting summaries markdown with verified timestamps: {md_file}")
    return md_file


def embed_with_candidate_image_timesteps_batch(batch_entries, batch_number, api_key, generated_batch_summary): 
    """
    For each time interval in the generated summaries, generate 5 potential candidate timestamps for images that provide meaningful visual context for the time interval. 

    Args: 
        batch_entries (list): List of transcript entries for this batch
        batch_number (int): Batch number for identification
        api_key (str): OpenAI API key
        generated_batch_summaries (list): List of batch summaries generated by the summarize_batch function

    Returns:
        string: a single string of timesteps for images of the format: 
        **Topic Title - Speaker Name** (H:MM:SS): [timestamps...]\n
    """ 

    if not api_key:
        return "API key not provided. Summaries not generated."

    # Extract batch text
    batch_text = extract_text_for_batch(batch_entries)

    if not batch_text.strip():
        return "No text available for summarization."

    # Get start and end times
    start_seconds = min(entry["seconds"] for entry in batch_entries)
    # End time is either explicit end_seconds or last entry
    if any("end_seconds" in entry for entry in batch_entries):
        # Use the max end_seconds if available
        end_seconds = max(
            entry.get("end_seconds", entry["seconds"]) for entry in batch_entries
        )
    else:
        # Otherwise use the last entry in the batch
        end_seconds = max(entry["seconds"] for entry in batch_entries)

    start_time = seconds_to_time_str(start_seconds)
    end_time = seconds_to_time_str(end_seconds)

    # Create a mapping of speaker names to ALL their timestamps for this batch
    speaker_timestamps = {}
    for entry in batch_entries:
        speaker = entry["name"]
        if speaker not in speaker_timestamps:
            speaker_timestamps[speaker] = []

        # Add this timestamp to the list for this speaker
        speaker_timestamps[speaker].append(
            {
                "seconds": entry["seconds"],
                "time_str": entry["time_str"],
                "text": entry["text"][:100],  # Include a snippet of text for context
            }
        )

    # Prepare the timestamp reference for the model
    timestamp_reference = "SPEAKER TIMESTAMPS (DO NOT MODIFY THESE):\n"
    for speaker, timestamps in speaker_timestamps.items():
        # Sort timestamps chronologically
        sorted_timestamps = sorted(timestamps, key=lambda x: x["seconds"])

        # Include all timestamps for the speaker with context snippets
        timestamp_reference += f"{speaker}:\n"
        for i, ts in enumerate(sorted_timestamps, 1):
            timestamp_reference += f"  {i}. {ts['time_str']} - '{ts['text']}...'\n"

    try:
        openai.api_key = api_key

        prompt = (
            "Given the generated summaries with time intervals, your task is to provide potential timestamps for each topic to take screenshots of a video recording. \n\n"
            "The screenshots should provide meaningful visual context for the content summaries. For each topic provided in the generated summary, you want to identify 5 potential candidate timestamps. \n\n"
            "OUTPUT FORMAT REQUIREMENTS (CRITICAL):\n"
            "1. Each topic must follow this EXACT format:\n"
            "   **Topic Title - Speaker Name** (H:MM:SS): [timestamps...]\n"
            "2. The format must be followed precisely with NO exceptions\n"
            "3. For instance an example of the output is: \n" 
            "   **Automated Meeting Minutes - John Doe* (3:05:00): [3:00:00, 3:01:00, 3:02:00, 3:03:00, 3:04:00]\n" 
            "   **Text Autonomy - Jane Smith** (3:10:00): [3:05:10, 3:05:20, 3:05:30, 3:05:40, 3:07:00]\n"
            "4. The order of topics in the output must match the order of the topics listed in the generated summary (DO NOT REORDER THE TOPICS)\n"
            "5. Do not include any other text in the output\n"
            "CANDIDATE TIMESTAMPS SELECTION RULES:\n"
            "1. Choose 5 potential distinct candidate timestamps for each topic provided in the generated summaries\n"
            "2. The 5 potential candidate timestamps must be directly relevant to the topic\n" 
            "3. The candidate timestamps for each topic must be in chronological order and must be in the format of H:MM:SS\n"
            "4. If possible, the candidate timestamps should correspond to the end of a notable action \n"
            "5. The candidate timestamps should be chosen to provide meaningful visual context for the content summaries\n"
            f"SPEAKER TIMESTAMPS (DO NOT MODIFY THESE):\n{timestamp_reference}\n\n"
            f"MEETING TRANSCRIPT BATCH #{batch_number} ({start_time} - {end_time}):\n\n{batch_text}"
            f"GENERATED SUMMARY:\n{generated_batch_summary}\n\n"
        )

        # Using chat completions API
        response = openai.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a technical meeting summarizer. NEVER modify the timestamps provided to you.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=1000,  # More tokens for batch summaries
        )
        image_embedded_timestamps = response.choices[0].message.content.strip()

        return image_embedded_timestamps

    except Exception as e:
        return f"Error generating image embedded timestamps: {str(e)}"
    

def evaluate_candidate_image_timesteps_across_all_batches(batch_summaries, image_timestamps, video_id, api_key):
    """
    For each time interval in the generated summaries, evaluate the candidate image timestamps and obtain image using video_id and timestamp. 
    Then use LLM to select up to 3 that provide the best visual context for the time interval. 

    Args: 
        batch_summaries (list): List of batch summaries generated by the summarize_batch function
        image_timestamps (list): List of image timestamps generated by the embed_with_candidate_image_timesteps_batch function
        video_id (str): Panopto video ID
        api_key (str): OpenAI API key

    Returns:
        dictionary: mapping each topic in the batch_summaries to an array of at most 3 images as a base64 string 
        the key to the dictionary is the topic title and speaker name
    """
    if not api_key:
        return "API key not provided. Image evaluation not performed."

    try:
        openai.api_key = api_key
        result = {}
        # key to result is the topic title and speaker name

        # Process each batch 
        for batch_number, (summary, timestamps) in enumerate(zip(batch_summaries, image_timestamps)):
            # Extract topics and their timestamps from the summary 
            topics = extract_topics_from_summary(summary, video_id)

            print(f"Processing Generated Timestamps of {timestamps} correspond to batch {batch_number}")

            # the ith topic of batch_summaries and the ith line of image_timestamps are expected to be aligned and correspond to the same topic and especially the same time interval
            for topic_idx, topic in enumerate(topics):
                # --- build your candidate list ---
                ts_lines = timestamps.split("\n")
                # index directly into image_timestamps and otherwise iterate over all lines of timestamps
                line = ts_lines[topic_idx] if topic_idx < len(ts_lines) and topic['topic'] in ts_lines[topic_idx] else None
                if not line or topic['topic'] not in line:
                    for ln in ts_lines:
                        if topic['topic'] in ln:
                            line = ln
                            break
                if not line:
                    continue

                m = re.search(r"\[(.*?)\]", line)
                if not m:
                    continue
                topic_timestamps = [ts.strip() for ts in m.group(1).split(",")]

                MAX_IMAGE_BYTES = 16384  # ~16 KB
                THUMBNAIL_SIZE = (128, 128)
                JPEG_QUALITY = 50

                timestamp_images = []
                actual_images = []
                for ts in topic_timestamps:
                    sec = time_str_to_seconds(ts)
                    print(f"→ requesting thumbnail for {ts} ({sec}s)")

                    # 1) Redirector URL
                    # redirect_url = (
                    #     f"https://mit.hosted.panopto.com/"
                    #     f"Panopto/Services/FrameGrabber.svc/FrameRedirect"
                    #     f"?objectId={video_id}"
                    #     f"&mode=Delivery"
                    #     f"&time={sec}"
                    # )
                    # r = requests.get(redirect_url, allow_redirects=False)
                    # print(f"  → Status: {r.status_code}")

                    img = screenshot_panopto_as_image(video_id, sec, resize_scale=RESIZE_SCALE) # resize scale factor of 0.5 
                    # create a copy of the image to store as JPEG bytes 
                    buf = io.BytesIO()
                    img.save(buf, format="JPEG", quality=JPEG_QUALITY)
                    actual_bytes = buf.getvalue()
                    actual_images.append({ 
                        "timestamp": ts,
                        "bytes": actual_bytes
                    })

                    # convert to thumbnail before passing into LLMs 
                    img.thumbnail(THUMBNAIL_SIZE)

                    if img.mode != "RGB":
                        img = img.convert("RGB")


                    buf = io.BytesIO()
                    img.save(buf, format="JPEG", quality=JPEG_QUALITY)
                    small_bytes = buf.getvalue()
                    

                    if len(small_bytes) <= MAX_IMAGE_BYTES:
                        timestamp_images.append({
                            "timestamp": ts,
                            "bytes": small_bytes
                        })
                    else:
                        print(f"    ✗ still too big, skipping {ts}")


                    # code instead to request the image from Panopto

                    # if r.status_code in (301, 302) and "Location" in r.headers:
                    #     image_url = r.headers["Location"]
                    #     print(f"  → Redirects to image URL: {image_url}")

                    #     # 2) Fetch the actual image
                    #     r2 = requests.get(image_url)
                    #     print(
                    #         f"    → final Status: {r2.status_code}, "
                    #         f"Content-Type: {r2.headers.get('Content-Type')}, "
                    #         f"bytes: {len(r2.content)}"
                    #     )

                    #     if r2.status_code == 200 and r2.headers.get("Content-Type", "").startswith("image/"):
                    #         # 3) Open & downscale
                    #         img = Image.open(io.BytesIO(r2.content))
                    #         img.thumbnail(THUMBNAIL_SIZE)

                    #         if img.mode != "RGB":
                    #             img = img.convert("RGB")

                    #         # 4) Re-encode as JPEG
                    #         buf = io.BytesIO()
                    #         img.save(buf, format="JPEG", quality=JPEG_QUALITY)
                    #         small_bytes = buf.getvalue()
                    #         print(f"    → thumbnail size: {len(small_bytes)} bytes")

                    #         # 5) Only keep it if <16 KB
                    #         if len(small_bytes) <= MAX_IMAGE_BYTES:
                    #             timestamp_images.append({
                    #                 "timestamp": ts,
                    #                 "bytes": small_bytes
                    #             })
                    #         else:
                    #             print(f"    ✗ still too big, skipping {ts}")
                    #     else:
                    #         print(f"    ✗ Failed to fetch valid image at {ts} (status {r2.status_code})")

                    # else:
                    #     # Unexpected response from redirector
                    # print("  ✗ No redirect! Body preview:", r.text[:200].replace("\n", " "))

                # If nothing made it through, skip downstream
                if not timestamp_images:
                    continue

                                # Prepare prompt for evaluating timestamps with image data
                prompt = (
                    "Given the following meeting summary and candidate timestamps as well as snapshots of the video at the candidate timestamps (named by their timestamp), "
                    "CRITICAL OUTPUT REQUIREMENTS: \n"
                    "These are rules that must be followed for the generation of the output:\n"
                    "1. Return ONLY the timestamps in H:MM:SS format and the description for each image (of the same index) as two separated arrays, separated by commas.\n"
                    "- Example output: [3:00:00, 3:01:00, 3:02:00], [description for the first image END** description for the second image END** description for the third image END**]\n"
                    "2. Do not include any other text in the output.\n"
                    "3. Only return timestamps that are provided as the candidate timestamps list and provide in the exact format of H:MM:SS.\n"
                    "4. Each description (element of the second array) MUST be elaborate and provide descriptions of each corresponding image. The sentences must be complete\n" 
                    "5. Each description MUST UTILIZE the full CONTEXT of the content, topic, and speaker which will be provided. It must be around 2-3 sentences each.\n"
                    "6. Select up to at most 3 timestamps that would provide the best visual context for the content. Consider:\n"
                        "- when key points are being discussed\n"
                        "- when visual aids or presentations are shown\n"
                        "- provide descriptions of the images that are relevant to the content\n"
                    "6. Return up to three timestamps and have less than three if some timestamps are not relevant to the content and do not provide meaningful visual context.\n"
                    f"Topic: {topic['topic']}\n"
                    f"Speaker: {topic['speaker']}\n"
                    f"Content: {topic['content']}\n"
                    f"Candidate timestamps: {', '.join(topic_timestamps)}\n\n"
                    "7. I will send you each candidate image as a separate message and each image is named by its timestamp\n"
                )

                # 3) Build a multimodal prompt
                messages = [
                    {
                        "role": "system",
                        "content": (
                            "You are a video content analyzer. "
                            "Choose up to 3 of the candidate timestamps below that "
                            "provide the best visual context."
                        )
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                            "type": "text",
                            "text": prompt
                            }
                        ]
                    }
                ]


                for img in timestamp_images:
                    base64_image = base64.b64encode(img["bytes"]).decode('utf-8')
                    # add text of the name of the image 
                    messages[1]["content"].append({
                        "type": "text",
                        "text": f"The name of the next image will be {img['timestamp']}.jpeg"
                    })
                    messages[1]["content"].append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                            "detail": "high"
                        }
                    })


                # 5) Ask the model to pick the best 3
                resp = openai.chat.completions.create(
                    model=MODEL,
                    messages=messages,
                    max_tokens=1000
                ) 

                # Parse the response which should be in format [ts1, ts2, ts3], [desc1, desc2, desc3]
                content = resp.choices[0].message.content.strip()
                
                # Extract the two arrays using regex
                timestamp_match = re.search(r'\[(.*?)\]', content)
                description_match = re.search(r'\[(.*?)\]', content[content.find('],') + 2:])
                
                if timestamp_match and description_match:
                    timestamps_processed = [ts.strip() for ts in timestamp_match.group(1).split(',')]
                    descriptions_processed = [desc.strip() for desc in description_match.group(1).split('END**')]
                else:
                    print(f"Warning: Could not parse response format: {content}")
                    timestamps_processed = []
                    descriptions_processed = []

                

                # sort picked in chronological order 
                sorted_pairs = sorted(zip(timestamps_processed, descriptions_processed), key=lambda x: time_str_to_seconds(x[0]))
                timestamps_processed, descriptions_processed = zip(*sorted_pairs)

                # 7) Collect back the images for the picked ones
                result_key = f"{topic['topic']} - {topic['speaker']}"
                result[result_key] = []

                # place in chronological order 
                # at most 5 candidates and choose up to 3 
                for timestamp, description in zip(timestamps_processed, descriptions_processed): 
                    # append images from actual images
                    for img in actual_images: 
                        if img["timestamp"] == timestamp: 
                            # for the given image with the timestamp, add the description 
                            img["description"] = description 
                            result[result_key].append(img)
                # at most 3 images 
                if len(result[result_key]) > 3: 
                    print(f"Warning: {result_key} has more than 3 images")
                    result[result_key] = result[result_key][:3]

        print(result.keys())
        return result

    except Exception as e:
        print(f"Error evaluating image timestamps: {str(e)}")
        return {}


def summarize_batch(batch_entries, batch_number, api_key):
    """
    Summarize a batch of transcript entries using OpenAI API

    Args:
        batch_entries (list): List of transcript entries for this batch
        batch_number (int): Batch number for identification
        api_key (str): OpenAI API key

    Returns:
        str: Batch summary with topics and timestamps
    """
    if not api_key:
        return "API key not provided. Summaries not generated."

    # Extract batch text
    batch_text = extract_text_for_batch(batch_entries)

    if not batch_text.strip():
        return "No text available for summarization."

    # Get start and end times
    start_seconds = min(entry["seconds"] for entry in batch_entries)
    # End time is either explicit end_seconds or last entry
    if any("end_seconds" in entry for entry in batch_entries):
        # Use the max end_seconds if available
        end_seconds = max(
            entry.get("end_seconds", entry["seconds"]) for entry in batch_entries
        )
    else:
        # Otherwise use the last entry in the batch
        end_seconds = max(entry["seconds"] for entry in batch_entries)

    start_time = seconds_to_time_str(start_seconds)
    end_time = seconds_to_time_str(end_seconds)

    # Create a mapping of speaker names to ALL their timestamps for this batch
    speaker_timestamps = {}
    for entry in batch_entries:
        speaker = entry["name"]
        if speaker not in speaker_timestamps:
            speaker_timestamps[speaker] = []

        # Add this timestamp to the list for this speaker
        speaker_timestamps[speaker].append(
            {
                "seconds": entry["seconds"],
                "time_str": entry["time_str"],
                "text": entry["text"][:100],  # Include a snippet of text for context
            }
        )

    # Prepare the timestamp reference for the model
    timestamp_reference = "SPEAKER TIMESTAMPS (DO NOT MODIFY THESE):\n"
    for speaker, timestamps in speaker_timestamps.items():
        # Sort timestamps chronologically
        sorted_timestamps = sorted(timestamps, key=lambda x: x["seconds"])

        # Include all timestamps for the speaker with context snippets
        timestamp_reference += f"{speaker}:\n"
        for i, ts in enumerate(sorted_timestamps, 1):
            timestamp_reference += f"  {i}. {ts['time_str']} - '{ts['text']}...'\n"

    try:
        openai.api_key = api_key

        # Construct prompt for batch summary with explicit timestamp instruction
        prompt = (
            "Your task is to create a structured summary of this meeting section.\n\n"
            "OUTPUT FORMAT REQUIREMENTS (CRITICAL):\n"
            "1. Each topic must follow this EXACT format:\n"
            "   **Topic Title - Speaker Name** (H:MM:SS): Content...\n"
            "2. The format must be followed precisely with NO exceptions\n"
            "3. Use only exact timestamps from the provided SPEAKER TIMESTAMPS section\n"
            "4. BOLD important terms within the content: **terms**\n"
            "5. Content should be in paragraph form (no bullet points or line breaks)\n\n"
            "TIMESTAMP SELECTION RULES:\n"
            "1. Choose the MOST RELEVANT timestamp from the provided options for each speaker\n"
            "2. Match the timestamp to where the specific topic is actually discussed\n"
            "3. NEVER create or modify timestamps - use only those provided\n\n"
            "CONTENT REQUIREMENTS:\n"
            "1. Thoroughly explain each key idea with technical precision\n"
            "2. Include interactions between different speakers\n"
            "3. Be detailed and comprehensive\n"
            "4. Do not hallucinate information\n"
            "5. Do not include a concluding summary paragraph\n\n"
            f"SPEAKER TIMESTAMPS (DO NOT MODIFY THESE):\n{timestamp_reference}\n\n"
            f"MEETING TRANSCRIPT BATCH #{batch_number} ({start_time} - {end_time}):\n\n{batch_text}"
        )

        # Using chat completions API
        response = openai.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "You are a technical meeting summarizer. NEVER modify the timestamps provided to you.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=10000,  # More tokens for batch summaries
        )

        summary = response.choices[0].message.content.strip()

        # Post-process to verify timestamps are from the provided list
        for speaker, timestamps in speaker_timestamps.items():
            # Create a set of valid timestamps for this speaker
            valid_timestamps = {ts["time_str"] for ts in timestamps}

            # Look for patterns like "**Topic - Speaker** (H:MM:SS):" with timestamps
            pattern = f"\\*\\*[^*]+ - {re.escape(speaker)}\\*\\* \\(([0-9]:[0-9]{{2}}:[0-9]{{2}})\\)"
            matches = re.finditer(pattern, summary)

            for match in matches:
                found_timestamp = match.group(1)

                # Check if the timestamp is valid for this speaker
                if found_timestamp not in valid_timestamps:
                    # Use the first timestamp as fallback
                    fallback_timestamp = timestamps[0]["time_str"]

                    # Replace the incorrect timestamp with a valid one
                    summary = summary.replace(
                        f"**{match.group(0).split('**')[1]}** ({found_timestamp})",
                        f"**{match.group(0).split('**')[1]}** ({fallback_timestamp})",
                    )
                    print(
                        f"Warning: Replaced invalid timestamp {found_timestamp} with {fallback_timestamp} for {speaker}"
                    )

        return summary

    except Exception as e:
        return f"Error generating batch summary: {str(e)}"


# -------------------------------------------------------------
# Main Processing Function
# -------------------------------------------------------------
"""
Optimized speaker summary generation integration for xlsx2html.py
"""

# Update the process_xlsx function in xlsx2html.py to use the optimized approach:


def process_xlsx(
    xlsx_file,
    video_id,
    html_file=None,
    speaker_summary_file=None,
    meeting_summary_md_file=None,
    batch_size_minutes=DEFAULT_BATCH_SIZE_MINUTES,
    use_enhanced_summaries=False,
):
    """
    Process Excel file to generate HTML links with summaries and meeting summaries

    Args:
        xlsx_file (str): Path to input Excel file
        video_id (str): Panopto video ID
        html_file (str, optional): Path to output HTML file for speaker links
        speaker_summary_file (str, optional): Path to output Markdown file for speaker summaries
        meeting_summary_md_file (str, optional): Path to output Markdown file for meeting summaries
        batch_size_minutes (int, optional): Batch size in minutes (default: DEFAULT_BATCH_SIZE_MINUTES)
        use_enhanced_summaries (bool, optional): Whether to use enhanced speaker summaries

    Returns:
        tuple: Paths to the generated files (html_file, summary_file, speaker_summary_file, meeting_summary_md_file)
    """
    if html_file is None:
        html_file = os.path.splitext(xlsx_file)[0] + "_speaker_summaries.html"

    if speaker_summary_file is None:
        speaker_summary_file = os.path.splitext(xlsx_file)[0] + "_speaker_summaries.md"

    summary_file = os.path.splitext(xlsx_file)[0] + "_meeting_summaries.html"

    if meeting_summary_md_file is None:
        meeting_summary_md_file = (
            os.path.splitext(xlsx_file)[0] + "_meeting_summaries.md"
        )

    # Ensure video_id is provided
    if not video_id:
        raise ValueError("Panopto video ID is required")

    # Get OpenAI API key
    api_key = get_api_key()
    if not api_key:
        print("Warning: OpenAI API key not provided. Summaries will not be generated.")
        return None, None, None, None

    try:
        # Read the Excel file
        df = pd.read_excel(xlsx_file)

        # Extract speaker links
        speaker_links = extract_unique_speakers(df)

        # Extract full transcript data
        transcript_data = extract_transcript_data(df)

        # Use enhanced speaker summaries if requested and available
        if use_enhanced_summaries and ENHANCED_SUMMARIES_AVAILABLE:
            print("Using enhanced speaker summaries with multiple topic support...")

            # Generate summaries data once - this avoids duplicate API calls
            print("Generating speaker topic summaries...")
            summaries_data = generate_speaker_summaries_data(transcript_data, api_key)

            # Generate enhanced speaker summary markdown
            if speaker_summary_file:
                generate_enhanced_speaker_summary_markdown(
                    transcript_data,
                    video_id,
                    speaker_summary_file,
                    api_key,
                    summaries_data,  # Pass the pre-generated summaries
                )
                print(
                    f"Generated enhanced speaker summary markdown: {speaker_summary_file}"
                )

            # Generate enhanced speaker summary HTML
            if html_file:
                generate_enhanced_speaker_summary_html(
                    transcript_data,
                    video_id,
                    html_file,
                    api_key,
                    summaries_data,  # Pass the pre-generated summaries
                )
                print(f"Generated enhanced speaker summary HTML: {html_file}")

        # Create time-based batches directly from transcript data
        print("Creating time-based batches for meeting summaries...")
        batches = create_time_batches(transcript_data, batch_size_minutes)
        print(f"Created {len(batches)} batches")

        # Generate batch summaries
        batch_summaries = []
        image_timestamps = []
        for i, batch in enumerate(batches, 1):
            # Get start and end times for this batch
            start_seconds = min(entry["seconds"] for entry in batch)
            # End time is either explicit end_seconds or last entry
            if any("end_seconds" in entry for entry in batch):
                end_seconds = max(
                    entry.get("end_seconds", entry["seconds"]) for entry in batch
                )
            else:
                end_seconds = max(entry["seconds"] for entry in batch)

            start_time = seconds_to_time_str(start_seconds)
            end_time = seconds_to_time_str(end_seconds)

            print(f"Processing batch {i}/{len(batches)}: {start_time} - {end_time}")

            # Generate summary
            summary = summarize_batch(batch, i, api_key)
            image_batch_timestamps = embed_with_candidate_image_timesteps_batch(batch, i, api_key, summary)
            batch_summaries.append(summary)
            image_timestamps.append(image_batch_timestamps)

        images_to_be_embedded_per_topic = evaluate_candidate_image_timesteps_across_all_batches(batch_summaries, image_timestamps, video_id, api_key)
        
        # Generate meeting summaries HTML with topic-level clickable links
        # Pass transcript_data for improved timestamp matching
        generate_meeting_summaries_html(
            batches, batch_summaries, video_id, summary_file, transcript_data, images_to_be_embedded_per_topic
        )
        print(f"Generated meeting summaries HTML: {summary_file}")

        # Generate meeting summaries Markdown with topic-level clickable links
        # Pass transcript_data for improved timestamp matching
        generate_meeting_summaries_markdown(
            batches, batch_summaries, video_id, meeting_summary_md_file, transcript_data, images_to_be_embedded_per_topic
        )
        print(f"Generated meeting summaries Markdown: {meeting_summary_md_file}")

        return html_file, summary_file, speaker_summary_file, meeting_summary_md_file

    except Exception as e:
        print(f"Error processing Excel file: {e}", file=sys.stderr)
        raise


def main():
    """
    Main function to handle command-line arguments and process Excel file
    """
    # Set up argument parser
    parser = argparse.ArgumentParser(
        description="Convert Excel transcript to HTML links with summaries"
    )
    parser.add_argument("input_file", help="Input Excel file")
    parser.add_argument("video_id", help="Panopto video ID (required)")
    parser.add_argument("output_file", nargs="?", help="Output HTML file (optional)")
    parser.add_argument(
        "--summary-file", help="Output file for meeting summaries HTML (optional)"
    )
    parser.add_argument(
        "--speaker-summary-file",
        help="Output file for speaker summaries markdown (optional)",
    )
    parser.add_argument(
        "--meeting-summary-md-file",
        help="Output file for meeting summaries markdown (optional)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=DEFAULT_BATCH_SIZE_MINUTES,
        help=f"Batch size in minutes (default: {DEFAULT_BATCH_SIZE_MINUTES})",
    )
    parser.add_argument(
        "--enhanced-summaries",
        action="store_true",
        help="Generate enhanced speaker summaries with multiple topics (requires speaker_summary_utils.py)",
    )

    args = parser.parse_args()

    # Check if enhanced summaries are requested but not available
    if args.enhanced_summaries and not ENHANCED_SUMMARIES_AVAILABLE:
        print(
            "Warning: Enhanced speaker summaries requested but speaker_summary_utils.py is not available."
        )
        print("Falling back to traditional speaker summaries.")
        args.enhanced_summaries = False

    try:
        html_file, summary_html_file, speaker_summary_file, meeting_summary_md_file = (
            process_xlsx(
                args.input_file,
                args.video_id,
                args.output_file,
                args.speaker_summary_file,
                args.meeting_summary_md_file,
                args.batch_size,
                args.enhanced_summaries,
            )
        )

        if html_file and summary_html_file:
            print(f"Processing complete!")
            print(f"Speaker links HTML: {html_file}")
            print(f"Speaker summary Markdown: {speaker_summary_file}")
            print(f"Meeting summaries HTML: {summary_html_file}")
            print(f"Meeting summaries Markdown: {meeting_summary_md_file}")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
